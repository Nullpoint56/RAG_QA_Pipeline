{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Generation based Question Answering pipeline\n",
    "Throughout this notebook, I'll show how I created a Retrieval Augmented Generation (RAG) pipeline for question answering over publicly available pdf and HTML data from PWC's website. Throughout the whole notebook, I will rely on Llama Index, an LLM application library. The notebook consists of 3 main chapters:\n",
    "1. Data Loading Pipeline: Assembling and running the pipeline that will load, transform and store our input data\n",
    "2. Question Answering Pipeline: Assembling and testing the pipeline that will generate answers for the posed questions\n",
    "\n",
    "Before we start, let's install the necessary libraries and initialize some variables that we will use throughout the notebook"
   ],
   "id": "ae7b1110413df4d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Installation of required packages\n",
    "Run the cell below. This cell should be ran only when you open this notebook for the very first time, after that, you don't have to run it."
   ],
   "id": "ec6c7250f2a775ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install --quiet llama-index llama-index-embeddings-ollama llama-index-llms-ollama llama-index-vector-stores-chroma llama-index-readers-file fitz pymupdf spacy nest-asyncio",
   "id": "9bd660de43c608d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initialize shared resources\n",
    "You need to run this cell every time you restart your kernel."
   ],
   "id": "aa3d3753273bb598"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T14:01:33.214626Z",
     "start_time": "2024-12-08T14:01:29.543628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline, DocstoreStrategy, IngestionCache\n",
    "from llama_index.legacy import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.readers.file import PyMuPDFReader, HTMLTagReader\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import chromadb\n",
    "import nest_asyncio\n",
    "\n",
    "# Set configuration for Ollama\n",
    "ollama_config = {\n",
    "    \"base_url\": \"127.0.0.1:11434\",\n",
    "    \"embedding_model_name\": \"nomic-embed-text\",\n",
    "    \"llm_name\": \"llama3.2\"\n",
    "}\n",
    "\n",
    "# Set project data paths\n",
    "project_data_paths = {\n",
    "    \"input_data_dir_path\": \"../data\",\n",
    "    \"evaluation_results_dir_path\": \"../results\",\n",
    "    \"vector_db_data_dir_path\": \"../vector_db_data\",\n",
    "    \"pipeline_cache_dir_path\": \"../pipeline_cache\",\n",
    "}\n",
    "\n",
    "# Define common resources\n",
    "embedding_model = OllamaEmbedding(base_url=ollama_config[\"base_url\"], model_name=ollama_config[\"embedding_model_name\"])\n",
    "llm = Ollama(model=ollama_config[\"llm_name\"], base_url=ollama_config[\"base_url\"])\n",
    "chroma_client = chromadb.PersistentClient(project_data_paths[\"vector_db_data_dir_path\"])\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"pwc_data\")\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "nest_asyncio.apply()\n"
   ],
   "id": "a05bcb0b20040564",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Loading Pipeline\n",
    "In this phase, we are going to load the PDF and HTML input documents.\n",
    "\n",
    "### What the code does\n",
    "- Instantiates and configures necessary objects to assemble the ingestion pipeline\n",
    "- Instantiates and configures the data ingestion pipeline\n",
    "\n",
    "### Loaders\n",
    "- PyMuPDFReader: Used to load PDF documents. I used this specific loader, because it relies on the well known PyMuPDF PDF parsing library, which is capable of identifying tables and other not necessarily text based object in the files. Another good choice would have been the SmartPDFLoader, but it requires llmsherpa backend service to be hosted, and it would further complicate the project. According to my previous experiences and the current performance of the loader, I found PyMuPDFReader is enough for now.\n",
    "- HTMLTagReader: Used to load HTML documents. It relies on the well known BeautifulSoup library, and it extracts text data from specified HTML tags, to filter out unused data, such as javascript scripts. \n",
    "\n",
    "### SimpleDirectoryReader\n",
    "This class is responsible to load files from a directory and use the specified readers to parse them. It supports external file systems too. In the code, I configured it to use the PyMuPDFReader for .pdf files and the HTMLTagReader for .html files. This way, I was able to create a unified loader, which meant both the pdf and html file's documents will be treated equally. Ofcourse, it is possible to treat them separately, but this is not necessary in our use-case, since the data the two file type holds can be considered to be in the same data domain, when we look at their content.\n",
    "\n",
    "### IngestionPipeline\n",
    "The ingestion pipeline is responsible to Transform the documents into Node objects, generate embeddings for them and store the Node-Embedding pairs in a vector database. Additionally, it manages a document store and a cache, so if it is run again using the same data, the pipeline will use the cached values instead of performing a full load again. (currently doesn't work for some reason).\n",
    "\n",
    "### SemanticSplitterNodeParser\n",
    "SemanticSplitterNodeParser is a Node parser, or a chunking method. I used it, because it doesn't use a static window size. Node parsers that are using static window sizes are unaware of internal document topics, and they easily combine text from two very different chapters. The problem with this, it that the chunk's embedding will be weak, uninformative, because it contains text from two so different topics and in worst case scenario, they might never be used in the RAG pipeline, because no query will ever be similar to it enough. In contrast, SemanticSplitterNodeParser creates the chunks by taking their meaning into account. It breaks the documents into sentences, generates embeddings to all of them and compares the embeddings of neighbour sentences to see how similar their embeddings are. If they are similar enough, they will be part of the same node/chunk, if not, a new node/chunk will be created and the sentence will be the first item of that node/chunk. Thanks to this clever approach, it is capable of producing dynamically sized chunks that encapsulate a topic in the document and by this, it is capable of chunking the document up without mushing internal document topics together.\n",
    "\n",
    "\n"
   ],
   "id": "8124a1cb3f0937f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:50:14.758733Z",
     "start_time": "2024-12-08T07:50:14.753921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define unified directory file loader\n",
    "pdf_reader = PyMuPDFReader()\n",
    "html_reader = HTMLTagReader(tag=\"section\", ignore_no_id=True)\n",
    "file_extractor = {\".pdf\": pdf_reader, \".html\": html_reader}\n",
    "document_reader = SimpleDirectoryReader(\n",
    "    input_dir=project_data_paths[\"input_data_dir_path\"], file_extractor=file_extractor\n",
    ")\n",
    "\n",
    "# Define unified document processing pipeline\n",
    "pwc_document_processing_pipeline = IngestionPipeline(\n",
    "    name=\"PWC document ingestion pipeline\",\n",
    "    project_name=\"PWC example project\",\n",
    "    docstore=SimpleDocumentStore(),\n",
    "    docstore_strategy=DocstoreStrategy.UPSERTS,\n",
    "    transformations=[SemanticSplitterNodeParser(embed_model=embedding_model), embedding_model],\n",
    "    vector_store=chroma_vector_store,\n",
    "    cache=IngestionCache()\n",
    ")"
   ],
   "id": "15b5921f62e7b5a0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lastly, we run the loading and transforming/storing pipeline",
   "id": "8f2c9be45c6a9d58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:50:21.482807Z",
     "start_time": "2024-12-08T07:50:15.659684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = document_reader.load_data(show_progress=True, num_workers=10)\n",
    "pwc_document_processing_pipeline.run(documents=documents, num_workers=10, cache_collection=\"pwc_cache\")\n",
    "pwc_document_processing_pipeline.persist(persist_dir=project_data_paths[\"pipeline_cache_dir_path\"])"
   ],
   "id": "8db11f7b430e749",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question Answering Pipeline\n",
    "In this phased, we create the pipeline that will generate the answers to the questions.\n",
    "\n",
    "### Answer generation architecture\n",
    "Here, we first create a VectorStoreIndex. The VectorStoreIndex is an object that provides access to the previously indexed and stored data inside the vector database. This is then used in a Query Engine component, which is responsible to orchestrate the response generation. When the pipeline is invoked, the Query Engine component first invokes the embedding model to embed the user's query. Then this embedding is passed to the VectorStoreIndex component, to retrieve the top 5 most similar chunks to the question embedding. After receiving the top 5 chunks, the QueryEngine invokes the ResponseSynthesizer module (which is by default part of the QueryEngine component) and concatenates these 5 chunks, then inserts it into a prompt, that it sends to the LLM for response generation. After receiving the answer, the QueryEngine component returns the response, which is the answer to our question.\n",
    "\n"
   ],
   "id": "428b6f423e2f916f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:29:35.467473Z",
     "start_time": "2024-12-08T07:29:35.460472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "pwc_vector_store_index = VectorStoreIndex.from_vector_store(\n",
    "    chroma_vector_store,\n",
    "    embed_model=embedding_model,\n",
    ")\n",
    "pwc_query_engine = pwc_vector_store_index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_topk=5\n",
    ")"
   ],
   "id": "215181cec9c09413",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Invoking the pipeline with a question the system should know the answer for.",
   "id": "c67157d9f320f4ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result = pwc_query_engine.query(\"Which country was the best in youth employment in 2024?\")",
   "id": "1cd743d39320fb76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:29:47.725391Z",
     "start_time": "2024-12-08T07:29:47.721983Z"
    }
   },
   "cell_type": "code",
   "source": "print(result)",
   "id": "19fb802610461ea0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Netherlands.\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
