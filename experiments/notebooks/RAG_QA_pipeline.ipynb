{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Generation based Question Answering pipeline\n",
    "\n",
    "Throughout this notebook, I'll show how I created a Retrieval Augmented Generation (RAG) pipeline for question answering over publicly available pdf and HTML data from PWC's website. Throughout the whole notebook, I will rely on Llama Index, an LLM application library. The notebook consists of 3 main chapters:\n",
    "\n",
    "1. Data Loading Pipeline: Assembling and running the pipeline that will load, transform and store our input data\n",
    "\n",
    "2. Question Answering Pipeline: Assembling and testing the pipeline that will generate answers for the posed questions\n",
    "\n",
    "\n",
    "Before we start, let's install the necessary libraries and initialize some variables that we will use throughout the notebook\n"
   ],
   "id": "ae7b1110413df4d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Installation of required packages\n",
    "\n",
    "Run the cell below. This cell should be ran only when you open this notebook for the very first time, after that, you don't have to run it.\n"
   ],
   "id": "ec6c7250f2a775ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install --quiet llama-index llama-index-embeddings-ollama llama-index-llms-ollama llama-index-vector-stores-chroma llama-index-readers-file fitz pymupdf spacy nest-asyncio",
   "id": "9bd660de43c608d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initialize shared resources\n",
    "\n",
    "You need to run this cell every time you restart your kernel.\n"
   ],
   "id": "aa3d3753273bb598"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T14:44:22.127929Z",
     "start_time": "2024-12-08T14:44:18.295838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline, DocstoreStrategy, IngestionCache\n",
    "from llama_index.legacy import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.readers.file import PyMuPDFReader, HTMLTagReader\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import chromadb\n",
    "\n",
    "# Set configuration for Ollama\n",
    "ollama_config = {\n",
    "    \"base_url\": \"127.0.0.1:11434\",\n",
    "    \"embedding_model_name\": \"nomic-embed-text\",\n",
    "    \"llm_name\": \"llama3.2\"\n",
    "}\n",
    "\n",
    "# Set project data paths\n",
    "project_data_paths = {\n",
    "    \"input_data_dir_path\": \"../data\",\n",
    "    \"evaluation_results_dir_path\": \"../results\",\n",
    "    \"vector_db_data_dir_path\": \"../vector_db_data\",\n",
    "    \"pipeline_cache_dir_path\": \"../pipeline_cache\",\n",
    "}\n",
    "\n",
    "# Define common resources\n",
    "embedding_model = OllamaEmbedding(base_url=ollama_config[\"base_url\"], model_name=ollama_config[\"embedding_model_name\"])\n",
    "llm = Ollama(model=ollama_config[\"llm_name\"], base_url=ollama_config[\"base_url\"])\n",
    "chroma_client = chromadb.PersistentClient(project_data_paths[\"vector_db_data_dir_path\"])\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"pwc_data\")\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n"
   ],
   "id": "a05bcb0b20040564",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Loading Pipeline\n",
    "\n",
    "In this phase, we load the PDF and HTML input documents.\n",
    "\n",
    "### What the Code Does\n",
    "- Instantiates and configures the necessary objects to assemble the ingestion pipeline.\n",
    "- Instantiates and configures the data ingestion pipeline.\n",
    "\n",
    "### Loaders\n",
    "- **PyMuPDFReader:**  \n",
    "  Used to load PDF documents. This loader relies on the well-known PyMuPDF PDF parsing library, which is capable of identifying tables and other non-text-based objects in the files. While another good choice could have been the SmartPDFLoader, it requires the `llmsherpa` backend service to be hosted, which would further complicate the project. Based on my prior experience and the current performance of this loader, I found PyMuPDFReader sufficient for now.\n",
    "  \n",
    "- **HTMLTagReader:**  \n",
    "  Used to load HTML documents. This loader utilizes the widely used BeautifulSoup library to extract text data from specified HTML tags, filtering out unused elements such as JavaScript scripts.\n",
    "\n",
    "### SimpleDirectoryReader\n",
    "This class is responsible for loading files from a directory and using the specified readers to parse them. It supports external file systems as well. In the code, I configured it to use PyMuPDFReader for `.pdf` files and HTMLTagReader for `.html` files. This setup creates a unified loader, ensuring that both PDF and HTML documents are treated equally. Of course, it is possible to treat them separately, but this is unnecessary in our use case, as the data in the two file types can be considered to belong to the same domain in terms of content.\n",
    "\n",
    "### IngestionPipeline\n",
    "The ingestion pipeline is responsible for:\n",
    "1. Transforming documents into **Node** objects.\n",
    "2. Generating embeddings for these nodes.\n",
    "3. Storing Node-Embedding pairs in a vector database.  \n",
    "\n",
    "Additionally, it manages a document store and a cache. If the pipeline is run again with the same data, it should use the cached values instead of performing a full reload (though this functionality currently does not work for some reason).\n",
    "\n",
    "### SemanticSplitterNodeParser\n",
    "The **SemanticSplitterNodeParser** is a node parser (or chunking method). I chose it because it avoids using a static window size. Parsers with static window sizes are unaware of internal document topics and often combine text from different sections, which weakens the embeddings. These mixed embeddings can lead to uninformative chunks, as the content comes from very different topics. In the worst case, such chunks might never be utilized in the RAG pipeline because no query will be similar enough to match them.\n",
    "\n",
    "In contrast, **SemanticSplitterNodeParser** creates chunks based on their semantic meaning. It breaks the document into sentences, generates embeddings for each sentence, and compares the embeddings of neighboring sentences to determine their similarity. If the sentences are similar enough, they form part of the same node (chunk). If not, a new node is created, and the sentence becomes the first item in that new node.\n",
    "\n",
    "This approach dynamically sizes chunks to encapsulate cohesive topics within the document. By doing so, it avoids combining unrelated sections and ensures that each chunk remains focused and meaningful, increasing its relevance and usability within the RAG pipeline.\n",
    "\n",
    "\n"
   ],
   "id": "8124a1cb3f0937f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:50:14.758733Z",
     "start_time": "2024-12-08T07:50:14.753921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define unified directory file loader\n",
    "pdf_reader = PyMuPDFReader()\n",
    "html_reader = HTMLTagReader(tag=\"section\", ignore_no_id=True)\n",
    "file_extractor = {\".pdf\": pdf_reader, \".html\": html_reader}\n",
    "document_reader = SimpleDirectoryReader(\n",
    "    input_dir=project_data_paths[\"input_data_dir_path\"], file_extractor=file_extractor\n",
    ")\n",
    "\n",
    "# Define unified document processing pipeline\n",
    "pwc_document_processing_pipeline = IngestionPipeline(\n",
    "    name=\"PWC document ingestion pipeline\",\n",
    "    project_name=\"PWC example project\",\n",
    "    docstore=SimpleDocumentStore(),\n",
    "    docstore_strategy=DocstoreStrategy.UPSERTS,\n",
    "    transformations=[SemanticSplitterNodeParser(embed_model=embedding_model), embedding_model],\n",
    "    vector_store=chroma_vector_store,\n",
    "    cache=IngestionCache()\n",
    ")"
   ],
   "id": "15b5921f62e7b5a0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lastly, we run the loading and transforming/storing pipeline",
   "id": "8f2c9be45c6a9d58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:50:21.482807Z",
     "start_time": "2024-12-08T07:50:15.659684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = document_reader.load_data(show_progress=True, num_workers=10)\n",
    "pwc_document_processing_pipeline.run(documents=documents, num_workers=10, cache_collection=\"pwc_cache\")\n",
    "pwc_document_processing_pipeline.persist(persist_dir=project_data_paths[\"pipeline_cache_dir_path\"])"
   ],
   "id": "8db11f7b430e749",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question Answering Pipeline\n",
    "\n",
    "In this phase, we create the pipeline that will generate answers to the questions.\n",
    "\n",
    "### Answer Generation Architecture\n",
    "\n",
    "Here, we first create a **VectorStoreIndex**. The VectorStoreIndex is an object that provides access to the previously indexed and stored data inside the vector database. This is then used in a **Query Engine** component, which is responsible for orchestrating the response generation.\n",
    "\n",
    "When the pipeline is invoked:\n",
    "1. The Query Engine component first invokes the embedding model to embed the user's query.\n",
    "2. This embedding is passed to the **VectorStoreIndex** component to retrieve the top 5 most similar chunks to the query embedding.\n",
    "3. After receiving the top 5 chunks, the Query Engine invokes the **ResponseSynthesizer** module (which is by default part of the Query Engine component). It concatenates these 5 chunks and inserts them into a prompt, which it sends to the LLM for response generation.\n",
    "4. After receiving the answer, the Query Engine component returns the response, which is the answer to the question.\n",
    "\n",
    "This pipeline ensures that all the necessary chunks for response generation are directly injected into the response generation prompt and used to augment the LLM appropriately.\n",
    "\n"
   ],
   "id": "428b6f423e2f916f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T14:44:23.251814Z",
     "start_time": "2024-12-08T14:44:22.137934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "pwc_vector_store_index = VectorStoreIndex.from_vector_store(\n",
    "    chroma_vector_store,\n",
    "    embed_model=embedding_model,\n",
    ")\n",
    "pwc_query_engine = pwc_vector_store_index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_topk=5,\n",
    ")"
   ],
   "id": "215181cec9c09413",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Invoking the pipeline with a question the system should know the answer for.",
   "id": "c67157d9f320f4ed"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-08T14:44:23.298187Z"
    }
   },
   "cell_type": "code",
   "source": "result = pwc_query_engine.query(\"Which country was the best in youth employment in 2024?\")",
   "id": "1cd743d39320fb76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:29:47.725391Z",
     "start_time": "2024-12-08T07:29:47.721983Z"
    }
   },
   "cell_type": "code",
   "source": "print(result)",
   "id": "19fb802610461ea0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Netherlands.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Final Thoughts\n",
    "\n",
    "### Why Llama Index?\n",
    "Llama Index is a versatile and efficient framework for building RAG applications. It is specifically designed for advanced indexing and knowledge management, whereas other LLM libraries excel in different areas. For example:\n",
    "- **LangChain** is far superior for creating and managing LLM applications, agentic behavior, or chatbot building.\n",
    "\n",
    "As demonstrated throughout the notebook, once its core abstractions are understood, Llama Index is relatively easy to use and requires significantly less boilerplate code than implementing such functionality from scratch. Additionally, it integrates well with other LLM frameworks, such as **LangChain** and **LiteLLM**.\n",
    "\n",
    "This application did not utilize all the advanced techniques the framework offers due to certain limitations, such as relying on **Ollama** because of my unsupported AMD GPU. Ollama does not support rerankers, which limited the framework's full potential. This brings us to the next question:\n",
    "\n",
    "### What Could Be Improved?\n",
    "\n",
    "1. **Auto Evaluation**  \n",
    "   Auto evaluation is the first thing I would implement. Currently, I couldn't figure out how to make auto evaluation work effectively. The **Llama 3.2 3b** model also struggled with creating a robust test set, which is crucial for conducting meaningful auto evaluation. I would have used metrics such as **faithfulness** and **correctness**. Additionally, I would have experimented with the **Ragas** framework for agent evaluation, but its integration currently suffers from a bug that has yet to be fixed.\n",
    "\n",
    "2. **Customization of Prompts**  \n",
    "   While the performance was acceptable, I am confident that with some prompt engineering, even the **Llama 3.2 3b** model could deliver more consistent responses.\n"
   ],
   "id": "15540cd391c11980"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
